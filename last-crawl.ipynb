{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-29T06:00:55.389943Z",
     "iopub.status.busy": "2025-07-29T06:00:55.389613Z",
     "iopub.status.idle": "2025-07-29T06:00:55.398729Z",
     "shell.execute_reply": "2025-07-29T06:00:55.397788Z",
     "shell.execute_reply.started": "2025-07-29T06:00:55.389912Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:/IT/crawl4AI/crawl_lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import asyncio\n",
    "from crawl4ai import *\n",
    "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n",
    "from crawl4ai.processors.pdf import PDFCrawlerStrategy, PDFContentScrapingStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo_basic_crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T06:02:03.760020Z",
     "iopub.status.busy": "2025-07-29T06:02:03.759257Z",
     "iopub.status.idle": "2025-07-29T06:02:03.767034Z",
     "shell.execute_reply": "2025-07-29T06:02:03.766264Z",
     "shell.execute_reply.started": "2025-07-29T06:02:03.759984Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "async def demo_basic_crawl():\n",
    "    print(\"\\n=== 1. Basic Web Crawbling ===\")\n",
    "\n",
    "    async with AsyncWebCrawler(config=BrowserConfig(\n",
    "        viewport_height=800,\n",
    "        viewport_width=1200,\n",
    "        headless=True,\n",
    "        verbose=True,\n",
    "    )) as crawler:\n",
    "        results: List[CrawlResult] = await crawler.arun(\n",
    "            url = 'https://tuoitre.vn/ba-truong-my-lan-bi-tuyen-y-an-tu-hinh-20241203120659551.htm'\n",
    "        )\n",
    "\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"Result {i+1}\")\n",
    "            print(f\"Success: {result.success}\")\n",
    "            if result.success:\n",
    "                print(f\"Markdown length: {len(result.markdown.raw_markdown)} chars\")\n",
    "                print(f\"100 first chars: {result.markdown.raw_markdown}...\")\n",
    "\n",
    "            else:\n",
    "                print(\"Failed to crawl the URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo_parallel_crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T06:02:03.768234Z",
     "iopub.status.busy": "2025-07-29T06:02:03.767914Z",
     "iopub.status.idle": "2025-07-29T06:02:03.788123Z",
     "shell.execute_reply": "2025-07-29T06:02:03.787168Z",
     "shell.execute_reply.started": "2025-07-29T06:02:03.768188Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "async def demo_parallel_crawl():\n",
    "    print(\"\\n=== 2. Parallel Crawling ===\")\n",
    "\n",
    "    urls = ['https://tuoitre.vn/ba-truong-my-lan-bi-tuyen-y-an-tu-hinh-20241203120659551.htm',\n",
    "           'https://tuoitre.vn/lanh-dao-tp-hcm-dang-huong-tuong-niem-cac-anh-hung-liet-si-tai-con-dao-20250727105240659.htm',\n",
    "           'https://tuoitre.vn/chung-cu-36-tang-tai-tp-hcm-nut-ho-o-tang-16-2025072710311524.htm']\n",
    "\n",
    "    async with AsyncWebCrawler(config=BrowserConfig(\n",
    "        viewport_height=800,\n",
    "        viewport_width=1200,\n",
    "        headless=True,\n",
    "        verbose=True,\n",
    "    )) as crawler:\n",
    "        results: List[CrawlResult] = await crawler.arun_many(\n",
    "            urls = urls\n",
    "        ) \n",
    "\n",
    "        print(f\"Crawled {len(results)} URLs in parallel:\")\n",
    "        for i, result in enumerate(results):\n",
    "            print(\n",
    "                f\" {i + i}. {result.url} - {'Success' if result.success else 'Failed'} \"\n",
    "            \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo_fit_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T06:02:03.789887Z",
     "iopub.status.busy": "2025-07-29T06:02:03.789197Z",
     "iopub.status.idle": "2025-07-29T06:02:03.810353Z",
     "shell.execute_reply": "2025-07-29T06:02:03.809492Z",
     "shell.execute_reply.started": "2025-07-29T06:02:03.789850Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "async def demo_fit_markdown():\n",
    "    print(\"\\n=== 3. Fit MarkDown with LLM content Filter ===\")\n",
    "\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        result: List[CrawlResult] = await crawler.arun(\n",
    "            url = 'https://ofac.treasury.gov/media/933656/download?inline',\n",
    "            config=CrawlerRunConfig(\n",
    "                markdown_generator=DefaultMarkdownGenerator(\n",
    "                    content_filter=PruningContentFilter()\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        print(f\"Raw: {len(result.markdown.raw_markdown)} chars\")\n",
    "        print(f\"Fit: {len(result.markdown.fit_markdown)} chars\")\n",
    "\n",
    "        print(f\"Fit markdown: {result.markdown.fit_markdown}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo_media_and_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T06:02:03.811609Z",
     "iopub.status.busy": "2025-07-29T06:02:03.811315Z",
     "iopub.status.idle": "2025-07-29T06:02:03.830026Z",
     "shell.execute_reply": "2025-07-29T06:02:03.829266Z",
     "shell.execute_reply.started": "2025-07-29T06:02:03.811583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "async def demo_media_and_links():\n",
    "    print(\"\\n=== 4. Media and links extraction ===\")\n",
    "\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        result: List[CrawlResult] = await crawler.arun(\n",
    "            url = 'https://tuoitre.vn/ba-truong-my-lan-bi-tuyen-y-an-tu-hinh-20241203120659551.htm'\n",
    "        )\n",
    "\n",
    "        for i, result in enumerate(result):\n",
    "\n",
    "            images = result.media.get(\"images\", [])\n",
    "            print(f\"Found {len(images)} images\")\n",
    "\n",
    "            internal_links = result.links.get(\"internal\", [])\n",
    "            external_links = result.links.get(\"external\", [])\n",
    "            print(f\"Found {len(internal_links)} internal links\")\n",
    "            print(f\"Found {len(external_links)} external links\")\n",
    "\n",
    "            # with open(\"images.json\", 'w') as f:\n",
    "            #     json.dump(images, f, indent = 2)\n",
    "\n",
    "            # with open(\"links.json\", 'w') as f:\n",
    "            #     json.dump(\n",
    "            #         {\"internal\": internal_links, \"external\": external_links},\n",
    "            #         f,\n",
    "            #         indent = 2\n",
    "            #     )\n",
    "\n",
    "            for image in images[:3]:\n",
    "                print(f\"Image: {image['src']}\")\n",
    "            for link in internal_links[:3]:\n",
    "                print(f\"Internal links: {link['href']}\")\n",
    "            for link in external_links[:3]:\n",
    "                print(f\"External links: {link['href']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo_pdf_crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T06:02:03.831945Z",
     "iopub.status.busy": "2025-07-29T06:02:03.831099Z",
     "iopub.status.idle": "2025-07-29T06:02:03.853030Z",
     "shell.execute_reply": "2025-07-29T06:02:03.852267Z",
     "shell.execute_reply.started": "2025-07-29T06:02:03.831921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "async def demo_pdf_crawl():\n",
    "    # Initialize the PDF crawler strategy\n",
    "    pdf_crawler_strategy = PDFCrawlerStrategy()\n",
    "\n",
    "    # PDFCrawlerStrategy is typically used in conjunction with PDFContentScrapingStrategy\n",
    "    # The scraping strategy handles the actual PDF content extraction\n",
    "    pdf_scraping_strategy = PDFContentScrapingStrategy()\n",
    "    run_config = CrawlerRunConfig(scraping_strategy=pdf_scraping_strategy)\n",
    "\n",
    "    async with AsyncWebCrawler(crawler_strategy=pdf_crawler_strategy) as crawler:\n",
    "        # Example with a remote PDF URL\n",
    "        pdf_url = \"https://ofac.treasury.gov/media/933901/download?inline\" # A public PDF from arXiv\n",
    "\n",
    "        print(f\"Attempting to process PDF: {pdf_url}\")\n",
    "        result = await crawler.arun(url=pdf_url, config=run_config)\n",
    "\n",
    "        if result.success:\n",
    "            print(f\"Successfully processed PDF: {result.url}\")\n",
    "            print(f\"Metadata Title: {result.metadata.get('title', 'N/A')}\")\n",
    "            # Further processing of result.markdown, result.media, etc.\n",
    "            # would be done here, based on what PDFContentScrapingStrategy extracts.\n",
    "            if result.markdown and hasattr(result.markdown, 'raw_markdown'):\n",
    "                print(f\"Extracted text: {result.markdown.raw_markdown}...\")\n",
    "            else:\n",
    "                print(\"No markdown (text) content extracted.\")\n",
    "        else:\n",
    "            print(f\"Failed to process PDF: {result.error_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... → Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. → Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m2\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to process PDF: https://ofac.treasury.gov/media/933901/download?inline\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... ↓ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://ofac.treasury.gov/media/933901/download?inline</span><span style=\"color: #008000; text-decoration-color: #008000\">                                               |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">00s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m ↓ \u001b[0m\u001b[4;32mhttps://ofac.treasury.gov/media/933901/download?inline\u001b[0m\u001b[32m                                               |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m00s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INFO</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... ℹ Downloading PDF from </span><span style=\"color: #008080; text-decoration-color: #008080; text-decoration: underline\">https://ofac.treasury.gov/media/933901/download?inline...</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINFO\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. ℹ Downloading PDF from \u001b[0m\u001b[4;36mhttps://ofac.treasury.gov/media/933901/download?inline...\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">[</span><span style=\"color: #808080; text-decoration-color: #808080\">DEBUG</span><span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">]</span><span style=\"color: #808080; text-decoration-color: #808080\">... ⋯ PDF download progress: </span><span style=\"color: #808080; text-decoration-color: #808080; font-weight: bold\">100</span><span style=\"color: #808080; text-decoration-color: #808080\">% </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;90m[\u001b[0m\u001b[90mDEBUG\u001b[0m\u001b[1;90m]\u001b[0m\u001b[90m...\u001b[0m\u001b[90m ⋯ PDF download progress: \u001b[0m\u001b[1;90m100\u001b[0m\u001b[90m% \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INFO</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... ℹ PDF downloaded successfully: C:\\Users\\batun\\AppData\\Local\\Temp\\tmp1z1hslq1.pdf </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINFO\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. ℹ PDF downloaded successfully: C:\\Users\\batun\\AppData\\Local\\Temp\\tmp1z1hslq1.pdf \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. ◆ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://ofac.treasury.gov/media/933901/download?inline</span><span style=\"color: #008000; text-decoration-color: #008000\">                                               |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">8.</span><span style=\"color: #008000; text-decoration-color: #008000\">73s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. ◆ \u001b[0m\u001b[4;32mhttps://ofac.treasury.gov/media/933901/download?inline\u001b[0m\u001b[32m                                               |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m8.\u001b[0m\u001b[32m73s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> ● </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://ofac.treasury.gov/media/933901/download?inline</span><span style=\"color: #008000; text-decoration-color: #008000\">                                               |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">✓ | ⏱: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">8.</span><span style=\"color: #008000; text-decoration-color: #008000\">73s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m ● \u001b[0m\u001b[4;32mhttps://ofac.treasury.gov/media/933901/download?inline\u001b[0m\u001b[32m                                               |\u001b[0m\n",
       "\u001b[32m✓\u001b[0m\u001b[32m | ⏱: \u001b[0m\u001b[1;32m8.\u001b[0m\u001b[32m73s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed PDF: https://ofac.treasury.gov/media/933901/download?inline\n",
      "Metadata Title: None\n",
      "Extracted text: **DEPARTMENT OF THE TREASURY**\n",
      "WASHINGTON, D.C.\n",
      "* * *\n",
      "**OFFICE OF FOREIGN ASSETS CONTROL**\n",
      "DETERMINATION PURSUANT TO SECTION 1(a)(ii) OF EXECUTIVE ORDER 140 71\n",
      "* * *\n",
      "Prohibition on Petroleum Services\n",
      "* * *\n",
      "Pursuant to sections 1(a)(ii), 1(b), and 5 of Execu tive Order (E.O.) 14071 of April 6, 2022 (“Prohibiting New Investment in and Certain Services to the Russian Federation in Response to Continued Russian Federation Aggression”) and 31 CFR § 587.802, and in consultation with the Department of State, I hereby determine that the prohibitions in section 1(a)(ii) of E.O. 14071 shall apply to the following categor y of services : petroleum services.\n",
      "* * *\n",
      "As a result, the following activities are prohibited, except to the extent provided by law, or unless licensed or otherwise authorized by the Office of Foreign Assets Control:\n",
      "* * *\n",
      "The exportation, reexportation, sale, or supply, directly or indirectly, from the United States, or by a United States person, wherever located, of petroleum services to any person located in the Russian Federation.\n",
      "* * *\n",
      "This determination excludes the following:\n",
      "* * *\n",
      "(1) any petroleum services related to isotopes derived from petroleum manufacturing that are used for medical, agricultural, or environmental purposes, such as C arbon -13;\n",
      "* * *\n",
      "(2) certain covered services related to the maritime transport of crude oil and petroleum products of Russia n Federation origin, provided that such crude oil or petroleum products are purchased at or below the relevant determined price caps, as specified in :\n",
      "* * *\n",
      "the Determination Pursuant to Sections 1(a)(ii), 1(b), and 5 of Executive Order 14071, Prohibitions on Certain Services as They Relate to the Maritime Transport of Crude Oil of Russian Federation Origin ;\n",
      "* * *\n",
      "the Determination Pursuant to Sections 1(a)(ii), 1(b), and 5 of Executive Order 14071, Prohibition s on Certain Services as They Relate to the Maritime Transport of Petroleum Products of Russian Federation Origin ;\n",
      "* * *\n",
      "the Determination Pursuant to Sections 1(a)(ii), 1(b), and 5 of Executive Order 14071, Price Cap on Crude Oil of Russian Federation Origin ; and\n",
      "* * *\n",
      "the Determination Pursuant to Sections 1(a)(ii), 1(b), and 5 of Executive Order 14071, Price Cap on Petroleum Products of Russian Federation Origin ;\n",
      "* * *\n",
      "(3) any service in connection with the wind down or divestiture of an entity located in the Russian Federation that is not owned or controlled, directly or indirectly, by a Russian person.\n",
      "* * *\n",
      "This determination shall take effect beginning at 12:01 a.m. eastern standard time on February 27, 2025.\n",
      "* * *\n",
      "___________________________ Lisa M. Palluconi Acting Director Office of Foreign Assets Control January 10, 2025\n",
      "* * *\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "await demo_pdf_crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo raw html and file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T06:02:03.854122Z",
     "iopub.status.busy": "2025-07-29T06:02:03.853874Z",
     "iopub.status.idle": "2025-07-29T06:02:03.875844Z",
     "shell.execute_reply": "2025-07-29T06:02:03.874944Z",
     "shell.execute_reply.started": "2025-07-29T06:02:03.854096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "async def demo_raw_html_and_file():\n",
    "    print(\"\\n=== 6. Raw HTML and local_files\")\n",
    "\n",
    "    raw_html = \"\"\"\n",
    "    <html> <body>\n",
    "        <h1> Sample Article </h1>\n",
    "        <p> This is sample content for testing Crawl4AI's raw HTML processing. </p>\n",
    "    </body></html>\n",
    "    \"\"\"\n",
    "\n",
    "    file_path = Path(\"docs/examples/tmp/sample.html\").absolute()\n",
    "\n",
    "    file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(raw_html)\n",
    "\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"raw: \" + raw_html, config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n",
    "        )\n",
    "        print(\"Raw HTML processing:\")\n",
    "        print(f\"Markdown: {result.markdown.raw_markdown[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo LLM Structured Extraction No Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T06:02:03.877387Z",
     "iopub.status.busy": "2025-07-29T06:02:03.876783Z",
     "iopub.status.idle": "2025-07-29T06:02:04.062735Z",
     "shell.execute_reply": "2025-07-29T06:02:04.061792Z",
     "shell.execute_reply.started": "2025-07-29T06:02:03.877361Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"GEMINI_API_KEY_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T06:02:04.063885Z",
     "iopub.status.busy": "2025-07-29T06:02:04.063607Z",
     "iopub.status.idle": "2025-07-29T06:02:04.071622Z",
     "shell.execute_reply": "2025-07-29T06:02:04.070774Z",
     "shell.execute_reply.started": "2025-07-29T06:02:04.063863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "async def demo_llm_structured_extraction_no_schema():\n",
    "\n",
    "    print(\"\\n=== 6. Extract Structured data with LLM ===\")\n",
    "    extraction_strategy = LLMExtractionStrategy(\n",
    "        llm_config=LLMConfig(\n",
    "            provider=\"gemini/gemini-2.5-flash\",\n",
    "            api_token=secret_value_0 \n",
    "        ),\n",
    "        instruction=(\n",
    "            \"This is link \"\n",
    "            \"Extract all news items you can find. I want: title, source url.\"\n",
    "        ),\n",
    "        extract_type=\"schema\",\n",
    "        schema=\"{title: string, url: string}\",\n",
    "\n",
    "        extract_args={\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 4096,\n",
    "        },\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    config = CrawlerRunConfig(extraction_strategy=extraction_strategy)\n",
    "\n",
    "    async with AsyncWebCrawler(config=BrowserConfig(\n",
    "        viewport_height=800,\n",
    "        viewport_width=1200,\n",
    "        headless=True,\n",
    "        verbose=True,\n",
    "    )) as crawler:\n",
    "        results: list[CrawlResult] = await crawler.arun(\n",
    "            url='https://vnexpress.net/',\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        for result in results:\n",
    "            print(f\"URL: {result.url}\")\n",
    "            print(f\"Success: {result.success}\")\n",
    "            if result.success:\n",
    "                try:\n",
    "                    data = json.loads(result.extracted_content)\n",
    "                    print(json.dumps(data, indent=2, ensure_ascii=False))\n",
    "                except Exception as e:\n",
    "                    print(\"Failed to parse extracted content:\", e)\n",
    "            else:\n",
    "                print(\"Failed to extract structured data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo js interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T06:02:04.074751Z",
     "iopub.status.busy": "2025-07-29T06:02:04.074460Z",
     "iopub.status.idle": "2025-07-29T06:02:04.096945Z",
     "shell.execute_reply": "2025-07-29T06:02:04.096012Z",
     "shell.execute_reply.started": "2025-07-29T06:02:04.074729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "async  def demo_js_interaction():\n",
    "    print(\"\\n=== 7. JavaScript Interaction === \")\n",
    "\n",
    "    async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:\n",
    "\n",
    "        news_schema = {\n",
    "            \"name\": \"news\",\n",
    "            \"baseSelector\": \"tr.athing\",\n",
    "            \"fields\":[\n",
    "                {\n",
    "                \"name\": \"title\",\n",
    "                \"selector\": \"span.titleline\",\n",
    "                \"type\": \"text\",\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        results: List[CrawlResult] = await crawler.arun(\n",
    "            url = 'https://ofac.treasury.gov/sanctions-programs-and-country-information/russian-harmful-foreign-activities-sanctions',\n",
    "            config=CrawlerRunConfig(\n",
    "                session_id=\"hn_session\",\n",
    "                extraction_strategy=JsonCssExtractionStrategy(schema=news_schema)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        news = []\n",
    "        for result in results:\n",
    "            if result.success:\n",
    "                data = json.loads(result.extracted_content)\n",
    "                news.extend(data)\n",
    "                print(json.dumps(data, indent=2))\n",
    "            else:\n",
    "                print(\"Failed to extract structured data\")\n",
    "        print(f\"Initial items: {len(news)}\")\n",
    "\n",
    "        more_config = CrawlerRunConfig(\n",
    "            js_code=\"document.querySelector('a.morelink').click():\",\n",
    "            js_only=True,\n",
    "            session_id=\"hn_session\",\n",
    "            extraction_strategy=JsonCssExtractionStrategy(\n",
    "                schema=news_schema\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        result: List[CrawlResult] = await crawler.arun(\n",
    "            url='https://ofac.treasury.gov/sanctions-programs-and-country-information/russian-harmful-foreign-activities-sanctions', config=more_config\n",
    "        )\n",
    "\n",
    "        for result in results:\n",
    "            if result.success:\n",
    "                data = json.loads(result.extracted_content)\n",
    "                news.extend(data)\n",
    "                print(json.dumps(data, indent=2))\n",
    "            else:\n",
    "                print(\"Failed to extract structured data\")\n",
    "        print(f\"Total items: {len(news)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo deep crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T06:02:04.098155Z",
     "iopub.status.busy": "2025-07-29T06:02:04.097851Z",
     "iopub.status.idle": "2025-07-29T06:02:04.120249Z",
     "shell.execute_reply": "2025-07-29T06:02:04.119326Z",
     "shell.execute_reply.started": "2025-07-29T06:02:04.098126Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "async def demo_deep_crawl():\n",
    "\n",
    "    print(\"\\n=== Deep Crawling\")\n",
    "    \n",
    "    filter_chain = FilterChain([DomainFilter(allowed_domains=[\"ofac.treasury.gov\"])])\n",
    "\n",
    "    deep_crawl_strategy = BFSDeepCrawlStrategy(\n",
    "        max_depth = 1, max_pages = 1000, filter_chain = filter_chain\n",
    "    )\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        results: List[CrawlResult] = await crawler.arun(\n",
    "            url='https://ofac.treasury.gov/sanctions-programs-and-country-information/russian-harmful-foreign-activities-sanctions',\n",
    "            config=CrawlerRunConfig(deep_crawl_strategy=deep_crawl_strategy),\n",
    "        )\n",
    "\n",
    "        print(f\"Deep crawl returned {len(results)} pages:\")\n",
    "        for i, result in enumerate(results):\n",
    "            depth = result.metadata.get(\"depth\", \"unknown\")\n",
    "            print(f\"{i+1}. {result.url} (Depth: {depth})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Deep Crawling\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m demo_deep_crawl()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mdemo_deep_crawl\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m filter_chain = FilterChain([DomainFilter(allowed_domains=[\u001b[33m\"\u001b[39m\u001b[33mofac.treasury.gov\u001b[39m\u001b[33m\"\u001b[39m])])\n\u001b[32m      7\u001b[39m deep_crawl_strategy = BFSDeepCrawlStrategy(\n\u001b[32m      8\u001b[39m     max_depth = \u001b[32m1\u001b[39m, max_pages = \u001b[32m1000\u001b[39m, filter_chain = filter_chain\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m AsyncWebCrawler() \u001b[38;5;28;01mas\u001b[39;00m crawler:\n\u001b[32m     11\u001b[39m     results: List[CrawlResult] = \u001b[38;5;28;01mawait\u001b[39;00m crawler.arun(\n\u001b[32m     12\u001b[39m         url=\u001b[33m'\u001b[39m\u001b[33mhttps://ofac.treasury.gov/sanctions-programs-and-country-information/russian-harmful-foreign-activities-sanctions\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     13\u001b[39m         config=CrawlerRunConfig(deep_crawl_strategy=deep_crawl_strategy),\n\u001b[32m     14\u001b[39m     )\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDeep crawl returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pages:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\IT/crawl4AI/crawl_lib\\crawl4ai\\async_webcrawler.py:194\u001b[39m, in \u001b[36mAsyncWebCrawler.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.start()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\IT/crawl4AI/crawl_lib\\crawl4ai\\async_webcrawler.py:177\u001b[39m, in \u001b[36mAsyncWebCrawler.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    171\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[33;03m    Start the crawler explicitly without using context manager.\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[33;03m    This is equivalent to using 'async with' but gives more control over the lifecycle.\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[33;03m        AsyncWebCrawler: The initialized crawler instance\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.crawler_strategy.\u001b[34m__aenter__\u001b[39m()\n\u001b[32m    178\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCrawl4AI \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcrawl4ai_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, tag=\u001b[33m\"\u001b[39m\u001b[33mINIT\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m.ready = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\IT/crawl4AI/crawl_lib\\crawl4ai\\async_crawler_strategy.py:111\u001b[39m, in \u001b[36mAsyncPlaywrightCrawlerStrategy.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.start()\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\IT/crawl4AI/crawl_lib\\crawl4ai\\async_crawler_strategy.py:121\u001b[39m, in \u001b[36mAsyncPlaywrightCrawlerStrategy.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    118\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[33;03m    Start the browser and initialize the browser manager.\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.browser_manager.start()\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execute_hook(\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mon_browser_created\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    124\u001b[39m         \u001b[38;5;28mself\u001b[39m.browser_manager.browser,\n\u001b[32m    125\u001b[39m         context=\u001b[38;5;28mself\u001b[39m.browser_manager.default_context,\n\u001b[32m    126\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\IT/crawl4AI/crawl_lib\\crawl4ai\\browser_manager.py:635\u001b[39m, in \u001b[36mBrowserManager.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplaywright\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masync_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m async_playwright\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m \u001b[38;5;28mself\u001b[39m.playwright = \u001b[38;5;28;01mawait\u001b[39;00m async_playwright().start()\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.cdp_url \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_managed_browser:\n\u001b[32m    638\u001b[39m     \u001b[38;5;28mself\u001b[39m.config.use_managed_browser = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\IT\\crawl4AI\\venv\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:51\u001b[39m, in \u001b[36mPlaywrightContextManager.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncPlaywright:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__aenter__\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\IT\\crawl4AI\\venv\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:46\u001b[39m, in \u001b[36mPlaywrightContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m playwright_future.done():\n\u001b[32m     45\u001b[39m     playwright_future.cancel()\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m playwright = AsyncPlaywright(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     47\u001b[39m playwright.stop = \u001b[38;5;28mself\u001b[39m.\u001b[34m__aexit__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m playwright\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\IT\\crawl4AI\\venv\\Lib\\site-packages\\playwright\\_impl\\_transport.py:120\u001b[39m, in \u001b[36mPipeTransport.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    117\u001b[39m         startupinfo.wShowWindow = subprocess.SW_HIDE\n\u001b[32m    119\u001b[39m     executable_path, entrypoint_path = compute_driver_executable()\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28mself\u001b[39m._proc = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_subprocess_exec(\n\u001b[32m    121\u001b[39m         executable_path,\n\u001b[32m    122\u001b[39m         entrypoint_path,\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrun-driver\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    124\u001b[39m         stdin=asyncio.subprocess.PIPE,\n\u001b[32m    125\u001b[39m         stdout=asyncio.subprocess.PIPE,\n\u001b[32m    126\u001b[39m         stderr=_get_stderr_fileno(),\n\u001b[32m    127\u001b[39m         limit=\u001b[32m32768\u001b[39m,\n\u001b[32m    128\u001b[39m         env=env,\n\u001b[32m    129\u001b[39m         startupinfo=startupinfo,\n\u001b[32m    130\u001b[39m     )\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_error_future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\subprocess.py:224\u001b[39m, in \u001b[36mcreate_subprocess_exec\u001b[39m\u001b[34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[39m\n\u001b[32m    221\u001b[39m loop = events.get_running_loop()\n\u001b[32m    222\u001b[39m protocol_factory = \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit=limit,\n\u001b[32m    223\u001b[39m                                                     loop=loop)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m transport, protocol = \u001b[38;5;28;01mawait\u001b[39;00m loop.subprocess_exec(\n\u001b[32m    225\u001b[39m     protocol_factory,\n\u001b[32m    226\u001b[39m     program, *args,\n\u001b[32m    227\u001b[39m     stdin=stdin, stdout=stdout,\n\u001b[32m    228\u001b[39m     stderr=stderr, **kwds)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\base_events.py:1794\u001b[39m, in \u001b[36mBaseEventLoop.subprocess_exec\u001b[39m\u001b[34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[39m\n\u001b[32m   1792\u001b[39m     debug_log = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mexecute program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   1793\u001b[39m     \u001b[38;5;28mself\u001b[39m._log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m transport = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_subprocess_transport(\n\u001b[32m   1795\u001b[39m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[32m   1796\u001b[39m     bufsize, **kwargs)\n\u001b[32m   1797\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1798\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, debug_log, transport)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\base_events.py:539\u001b[39m, in \u001b[36mBaseEventLoop._make_subprocess_transport\u001b[39m\u001b[34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_subprocess_transport\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol, args, shell,\n\u001b[32m    536\u001b[39m                                      stdin, stdout, stderr, bufsize,\n\u001b[32m    537\u001b[39m                                      extra=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    538\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "await demo_deep_crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T06:02:04.121775Z",
     "iopub.status.busy": "2025-07-29T06:02:04.121441Z",
     "iopub.status.idle": "2025-07-29T06:02:04.142754Z",
     "shell.execute_reply": "2025-07-29T06:02:04.141858Z",
     "shell.execute_reply.started": "2025-07-29T06:02:04.121749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "async def crawl_and_extract():\n",
    "    filter_chain = FilterChain([DomainFilter(allowed_domains=[\"vnexpress.net\"])])\n",
    "\n",
    "    # Deep Crawling\n",
    "    deep_crawl_strategy = BFSDeepCrawlStrategy(\n",
    "        max_depth = 1, max_pages = 1000, filter_chain = filter_chain\n",
    "    )\n",
    "\n",
    "    # Fit markdown\n",
    "    markdown_gen = DefaultMarkdownGenerator(\n",
    "        content_filter=PruningContentFilter()\n",
    "    )\n",
    "\n",
    "    # LLM\n",
    "    extraction_strategy = LLMExtractionStrategy(\n",
    "        llm_config=LLMConfig(\n",
    "            provider=\"gemini/gemini-2.5-flash\",\n",
    "            api_token=\"AIzaSyCwmGF3qjto6g03a_9am0Rc5L4HewDrysc\"\n",
    "        ),\n",
    "        instruction=(\n",
    "            \"This is link \"\n",
    "            \"Extract all news items you can find. I want: title, source url.\"\n",
    "        ),\n",
    "        extract_type=\"schema\",\n",
    "        schema=\"{title: string, url: string}\",\n",
    "\n",
    "        extract_args={\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 4096,\n",
    "        },\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Combine \n",
    "    run_cfg = CrawlerRunConfig(\n",
    "        deep_crawl_strategy=deep_crawl_strategy,\n",
    "        markdown_generator=markdown_gen,\n",
    "        # extraction_strategy=extraction_strategy\n",
    "    )\n",
    "\n",
    "    async with AsyncWebCrawler(config=BrowserConfig(\n",
    "        viewport_height=800,\n",
    "        viewport_width=1200,\n",
    "        headless=True,\n",
    "        verbose=True,\n",
    "    )) as crawler:\n",
    "        results: List[CrawlResult] = await crawler.arun(\n",
    "            url='https://vnexpress.net/',\n",
    "            config=run_cfg,\n",
    "        )\n",
    "        \n",
    "        for res in results:\n",
    "            print(\"URL:\", res.url)\n",
    "            print(\"Depth:\", res.metadata.get(\"depth\"))\n",
    "            print(\"Extracted ok:\", res.success)\n",
    "            if res.success:\n",
    "                # res.extracted_content là JSON string theo schema\n",
    "                print(res.extracted_content)\n",
    "            else:\n",
    "                print(\"  — No data extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T06:02:04.144470Z",
     "iopub.status.busy": "2025-07-29T06:02:04.143936Z",
     "iopub.status.idle": "2025-07-29T06:05:04.408513Z",
     "shell.execute_reply": "2025-07-29T06:05:04.407599Z",
     "shell.execute_reply.started": "2025-07-29T06:02:04.144446Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# await demo_basic_crawl()\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# await demo_parallel_crawl()\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# await demo_fit_markdown()\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# await demo_js_interaction()\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# await demo_deep_crawl()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m crawl_and_extract()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mcrawl_and_extract\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Combine \u001b[39;00m\n\u001b[32m     35\u001b[39m run_cfg = CrawlerRunConfig(\n\u001b[32m     36\u001b[39m     deep_crawl_strategy=deep_crawl_strategy,\n\u001b[32m     37\u001b[39m     markdown_generator=markdown_gen,\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# extraction_strategy=extraction_strategy\u001b[39;00m\n\u001b[32m     39\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m AsyncWebCrawler() \u001b[38;5;28;01mas\u001b[39;00m crawler:\n\u001b[32m     42\u001b[39m     results: List[CrawlResult] = \u001b[38;5;28;01mawait\u001b[39;00m crawler.arun(\n\u001b[32m     43\u001b[39m         url=\u001b[33m'\u001b[39m\u001b[33mhttps://vnexpress.net/\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     44\u001b[39m         config=run_cfg,\n\u001b[32m     45\u001b[39m     )\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\IT/crawl4AI/crawl_lib\\crawl4ai\\async_webcrawler.py:194\u001b[39m, in \u001b[36mAsyncWebCrawler.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.start()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\IT/crawl4AI/crawl_lib\\crawl4ai\\async_webcrawler.py:177\u001b[39m, in \u001b[36mAsyncWebCrawler.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    171\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[33;03m    Start the crawler explicitly without using context manager.\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[33;03m    This is equivalent to using 'async with' but gives more control over the lifecycle.\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[33;03m        AsyncWebCrawler: The initialized crawler instance\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.crawler_strategy.\u001b[34m__aenter__\u001b[39m()\n\u001b[32m    178\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCrawl4AI \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcrawl4ai_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, tag=\u001b[33m\"\u001b[39m\u001b[33mINIT\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m.ready = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\IT/crawl4AI/crawl_lib\\crawl4ai\\async_crawler_strategy.py:111\u001b[39m, in \u001b[36mAsyncPlaywrightCrawlerStrategy.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.start()\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\IT/crawl4AI/crawl_lib\\crawl4ai\\async_crawler_strategy.py:121\u001b[39m, in \u001b[36mAsyncPlaywrightCrawlerStrategy.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    118\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[33;03m    Start the browser and initialize the browser manager.\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.browser_manager.start()\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execute_hook(\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mon_browser_created\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    124\u001b[39m         \u001b[38;5;28mself\u001b[39m.browser_manager.browser,\n\u001b[32m    125\u001b[39m         context=\u001b[38;5;28mself\u001b[39m.browser_manager.default_context,\n\u001b[32m    126\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\IT/crawl4AI/crawl_lib\\crawl4ai\\browser_manager.py:635\u001b[39m, in \u001b[36mBrowserManager.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplaywright\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masync_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m async_playwright\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m \u001b[38;5;28mself\u001b[39m.playwright = \u001b[38;5;28;01mawait\u001b[39;00m async_playwright().start()\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.cdp_url \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_managed_browser:\n\u001b[32m    638\u001b[39m     \u001b[38;5;28mself\u001b[39m.config.use_managed_browser = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\IT\\crawl4AI\\venv\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:51\u001b[39m, in \u001b[36mPlaywrightContextManager.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncPlaywright:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__aenter__\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\IT\\crawl4AI\\venv\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:46\u001b[39m, in \u001b[36mPlaywrightContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m playwright_future.done():\n\u001b[32m     45\u001b[39m     playwright_future.cancel()\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m playwright = AsyncPlaywright(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     47\u001b[39m playwright.stop = \u001b[38;5;28mself\u001b[39m.\u001b[34m__aexit__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m playwright\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\IT\\crawl4AI\\venv\\Lib\\site-packages\\playwright\\_impl\\_transport.py:120\u001b[39m, in \u001b[36mPipeTransport.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    117\u001b[39m         startupinfo.wShowWindow = subprocess.SW_HIDE\n\u001b[32m    119\u001b[39m     executable_path, entrypoint_path = compute_driver_executable()\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28mself\u001b[39m._proc = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_subprocess_exec(\n\u001b[32m    121\u001b[39m         executable_path,\n\u001b[32m    122\u001b[39m         entrypoint_path,\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrun-driver\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    124\u001b[39m         stdin=asyncio.subprocess.PIPE,\n\u001b[32m    125\u001b[39m         stdout=asyncio.subprocess.PIPE,\n\u001b[32m    126\u001b[39m         stderr=_get_stderr_fileno(),\n\u001b[32m    127\u001b[39m         limit=\u001b[32m32768\u001b[39m,\n\u001b[32m    128\u001b[39m         env=env,\n\u001b[32m    129\u001b[39m         startupinfo=startupinfo,\n\u001b[32m    130\u001b[39m     )\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_error_future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\subprocess.py:224\u001b[39m, in \u001b[36mcreate_subprocess_exec\u001b[39m\u001b[34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[39m\n\u001b[32m    221\u001b[39m loop = events.get_running_loop()\n\u001b[32m    222\u001b[39m protocol_factory = \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit=limit,\n\u001b[32m    223\u001b[39m                                                     loop=loop)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m transport, protocol = \u001b[38;5;28;01mawait\u001b[39;00m loop.subprocess_exec(\n\u001b[32m    225\u001b[39m     protocol_factory,\n\u001b[32m    226\u001b[39m     program, *args,\n\u001b[32m    227\u001b[39m     stdin=stdin, stdout=stdout,\n\u001b[32m    228\u001b[39m     stderr=stderr, **kwds)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\base_events.py:1794\u001b[39m, in \u001b[36mBaseEventLoop.subprocess_exec\u001b[39m\u001b[34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[39m\n\u001b[32m   1792\u001b[39m     debug_log = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mexecute program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   1793\u001b[39m     \u001b[38;5;28mself\u001b[39m._log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m transport = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_subprocess_transport(\n\u001b[32m   1795\u001b[39m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[32m   1796\u001b[39m     bufsize, **kwargs)\n\u001b[32m   1797\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1798\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, debug_log, transport)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\base_events.py:539\u001b[39m, in \u001b[36mBaseEventLoop._make_subprocess_transport\u001b[39m\u001b[34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_subprocess_transport\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol, args, shell,\n\u001b[32m    536\u001b[39m                                      stdin, stdout, stderr, bufsize,\n\u001b[32m    537\u001b[39m                                      extra=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    538\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# await demo_basic_crawl()\n",
    "# await demo_parallel_crawl()\n",
    "# await demo_fit_markdown()\n",
    "# await demo_media_and_links()\n",
    "# await demo_pdf_crawl()\n",
    "# await demo_llm_structured_extraction_no_schema()\n",
    "# await demo_raw_html_and_file()\n",
    "# await demo_js_interaction()\n",
    "# await demo_deep_crawl()\n",
    "await crawl_and_extract()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
