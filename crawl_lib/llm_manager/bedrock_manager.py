import boto3
import json
from llm_manager.model_manager import LlmModelManager
from utils import Utils
import config
from botocore.exceptions import ClientError
from logger import _setup_logger
import re
import time 

logger = _setup_logger(__name__, config.LOG_LEVEL)

class BedrockModelManager(LlmModelManager):
    def __init__(
        self,
        aws_access_key_id: str,
        aws_secret_access_key: str,
        region_name: str = "us-east-1",
        default_model_id: str = "anthropic.claude-3-haiku-20240307-v1:0"
    ):
        self.client = boto3.client(
            "bedrock-runtime",
            aws_access_key_id=aws_access_key_id,
            aws_secret_access_key=aws_secret_access_key,
            region_name=region_name
        )
        self.default_model_id = default_model_id

    def get_model(self, model_name: str = None) -> str:
        return model_name or self.default_model_id

    def extract_thinking_and_answer(self, response: str):
        """
        T√°ch ph·∫ßn tr∆∞·ªõc (g·ªìm <think>...</think>) v√† ph·∫ßn sau </think> t·ª´ m·ªôt chu·ªói vƒÉn b·∫£n.
        
        Tr·∫£ v·ªÅ:
            thinking_part (str): n·ªôi dung t·ª´ ƒë·∫ßu ƒë·∫øn h·∫øt </think>
            json_part (str): ph·∫ßn sau </think>
        """
        match = re.search(r"(.*?</think>)(.*)", response, re.DOTALL)
        if not match:
            raise ValueError("Kh√¥ng t√¨m th·∫•y th·∫ª </think> trong chu·ªói.")
        
        thinking = match.group(1).strip()
        answer = match.group(2).strip()
        return answer, thinking
    
    def generate_deepseek(
        self,
        prompt: str,
        max_tokens: int = 12000,
        temperature: float = 0.5,
        top_p: float = 0.9,
        model_id: str = "us.deepseek.r1-v1:0"
    ) -> str:
        """
        G·ª≠i prompt ƒë·∫øn m√¥ h√¨nh DeepSeek-R1 v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ text (ho·∫∑c raise l·ªói n·∫øu th·∫•t b·∫°i sau 3 l·∫ßn).
        """

        logger.info(f"üöÄ ƒêang g·ªçi m√¥ h√¨nh DeepSeek...{model_id}")
        formatted_prompt = f"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>{prompt}<ÔΩúAssistantÔΩú><think>\n"

        body = json.dumps({
            "prompt": formatted_prompt,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p
        })

        max_retries = 3
        for attempt in range(max_retries + 1):
            try:
                response = self.client.invoke_model(
                    modelId=model_id,
                    body=body,
                    contentType="application/json",
                    accept="application/json"
                )
                model_response = json.loads(response["body"].read())
                choices = model_response.get("choices", [])

                if not choices:
                    raise RuntimeError("No choices returned from DeepSeek.")

                resp = choices[0]["text"]
                answer, thinking = self.extract_thinking_and_answer(resp)

                logger.debug(f"[DeepSeek] ƒê√£ nh·∫≠n ph·∫£n h·ªìi: {answer}")
                if thinking:
                    logger.debug(f"[DeepSeek] Reasoning: {thinking}")
                else:
                    logger.debug("[DeepSeek] Kh√¥ng c√≥ reasoning.")

                return answer, thinking

            except (ClientError, Exception) as e:
                logger.warning(f"[DeepSeek] Th·ª≠ l·∫ßn {attempt + 1} th·∫•t b·∫°i: {e}")
                if attempt < max_retries:
                    time.sleep(10 ** attempt)  # exponential backoff: 1s, 2s
                else:
                    raise RuntimeError(f"[DeepSeek] L·ªói sau {max_retries + 1} l·∫ßn th·ª≠: {e}")

    def generate_claude(
        self,
        prompt: str,
        model_name: str = None,
        max_token: int = 50000,
        temperature: float = 0.5,
        enable_thinking: bool = False,
        thinking_budget_tokens: int = 2000,
        max_retries: int = 3,
        retry_delay: int = 2  # gi√¢y
    ) -> str:
        
        logger.info(f"üöÄ ƒêang g·ªçi m√¥ h√¨nh Bedrock...{model_name}")
        model_id = self.get_model(model_name)

        # Claude 3 request format
        body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": max_token,
            "temperature": temperature,
            "messages": [
                {
                    "role": "user",
                    "content": [{"type": "text", "text": prompt}]
                }
            ]
        }

        if enable_thinking:
            body['temperature'] = 1
            body["thinking"] = {
                "type": "enabled",
                "budget_tokens": thinking_budget_tokens
            }

        for attempt in range(1, max_retries + 1):
            try:
                logger.debug(f"üîÅ G·ªçi Bedrock (th·ª≠ l·∫ßn {attempt}) model {model_id}")
                response = self.client.invoke_model(
                    modelId=model_id,
                    body=json.dumps(body),
                    accept="application/json",
                    contentType="application/json"
                )
                break  # Th√†nh c√¥ng, tho√°t v√≤ng l·∫∑p

            except (ClientError, Exception) as e:
                logger.warning(f"‚ùå L·ªói khi g·ªçi m√¥ h√¨nh {model_id} (th·ª≠ l·∫ßn {attempt}): {e}")
                if attempt == max_retries:
                    raise RuntimeError(f"üö® ƒê√£ th·ª≠ {max_retries} l·∫ßn nh∆∞ng kh√¥ng th√†nh c√¥ng: {e}")
                time.sleep(retry_delay)

        result = json.loads(response["body"].read())

        content_blocks = result.get("content", [])
        reasoning_text = None
        final_text = None

        for block in content_blocks:
            if block.get("type") == "thinking":
                reasoning_text = block.get("thinking")
            elif block.get("type") == "text":
                final_text = block.get("text")

        if reasoning_text:
            logger.debug(f"[generate] ‚úÖ ƒê√£ nh·∫≠n ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh {model_id} v·ªõi reasoning.")
            logger.debug(f"[generate] Reasoning: {reasoning_text}")
            return final_text, reasoning_text
        else:
            logger.debug(f"[generate] ‚úÖ ƒê√£ nh·∫≠n ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh {model_id} m√† kh√¥ng c√≥ reasoning.")
            return final_text, ""

    def generate_amazon(
            self,
            prompt: str,
            model_id: str = "amazon.titan-nova-pro-v1",  # C·∫≠p nh·∫≠t model_id ch√≠nh x√°c
            temperature: float = 0.5,
            top_p: float = 0.9
        ) -> tuple[str, str]:
            """
            G·ª≠i prompt ƒë·∫øn m√¥ h√¨nh Amazon Titan Nova Pro v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ (answer, reasoning="").
            L∆∞u √Ω: S·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng messages cho Amazon Bedrock API.
            """
            logger.info(f"üöÄ ƒêang g·ªçi m√¥ h√¨nh Amazon Titan Nova Pro... {model_id}")

            # ƒê·ªãnh d·∫°ng body theo y√™u c·∫ßu c·ªßa Amazon Bedrock
            body = {
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": max(0.0, min(temperature, 1.0)),  # ƒê·∫£m b·∫£o temperature trong kho·∫£ng h·ª£p l·ªá
                "topP": max(0.0, min(top_p, 1.0)),  # Th√™m top_p v√†o body
                "maxTokens": 2048  # Gi·ªõi h·∫°n s·ªë token t·ªëi ƒëa, c√≥ th·ªÉ t√πy ch·ªânh
            }

            try:
                # G·ªçi API Amazon Bedrock
                response = self.client.invoke_model(
                    modelId=model_id,
                    body=json.dumps(body),
                    contentType="application/json",
                    accept="application/json"
                )
                
                # Gi·∫£i m√£ ph·∫£n h·ªìi
                result = json.loads(response["body"].read().decode("utf-8"))
                # L·∫•y n·ªôi dung t·ª´ ph·∫£n h·ªìi, gi·∫£ s·ª≠ ƒë·ªãnh d·∫°ng ph·∫£n h·ªìi c√≥ tr∆∞·ªùng 'choices'
                output_text = result.get("choices", [{}])[0].get("message", {}).get("content", "").strip()

                logger.debug(f"[Amazon Titan Nova Pro] ‚úÖ Ph·∫£n h·ªìi: {output_text}")
                return output_text, ""  # Kh√¥ng c√≥ reasoning

            except ClientError as e:
                error_code = e.response.get("Error", {}).get("Code")
                error_message = e.response.get("Error", {}).get("Message")
                logger.error(f"‚ùå L·ªói ClientError khi g·ªçi m√¥ h√¨nh Amazon Titan Nova Pro {model_id}: {error_code} - {error_message}")
                raise RuntimeError(f"‚ùå L·ªói khi g·ªçi m√¥ h√¨nh Amazon Titan Nova Pro {model_id}: {error_message}")
            except Exception as e:
                logger.error(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh khi g·ªçi m√¥ h√¨nh Amazon Titan Nova Pro {model_id}: {str(e)}")
                raise RuntimeError(f"‚ùå L·ªói khi g·ªçi m√¥ h√¨nh Amazon Titan Nova Pro {model_id}: {str(e)}")

    def generate(
        self,
        prompt: str,
        model_type: str,  # ho·∫∑c "claude"
        model_name: str = None,
        max_tokens: int = 12000,
        temperature: float = 1,
        top_p: float = 0.9,
        enable_thinking: bool = False,
        thinking_budget_tokens: int = 2000
    ) -> tuple[str, str]:
        """
        G·ªçi m√¥ h√¨nh t∆∞∆°ng ·ª©ng theo model_type v√† tr·∫£ v·ªÅ (answer, reasoning).
        """
        start = time.time()
        if model_type.lower() == "deepseek":
            result = self.generate_deepseek(
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p
            )

        elif model_type.lower() == "claude":
            result =  self.generate_claude(
                prompt=prompt,
                model_name=model_name,
                max_token=max_tokens,
                temperature=temperature,
                enable_thinking=enable_thinking,
                thinking_budget_tokens=thinking_budget_tokens
            )
        elif model_type.lower() == "amazon":
            result =  self.generate_amazon(
                prompt=prompt,
                model_id=model_name or "amazon.nova-pro-v1:0",
                temperature=temperature,
                top_p=top_p
            ) 
        else:
            raise ValueError(f"Unsupported model_type: {model_type}")
        
        end = time.time()
        logger.info(f"[generate] Th·ªùi gian g·ªçi m√¥ h√¨nh {model_type} ({model_name}): {end - start:.2f} gi√¢y")
        return result


def main():
    import json

    # C·∫•u h√¨nh c·ªë ƒë·ªãnh
    AWS_ACCESS_KEY = Utils.load_api_key_from_env("AWS_ACCESS_KEY")
    AWS_SECRET_KEY = Utils.load_api_key_from_env("AWS_SECRET_KEY")
    REGION = config.AWS_VIRGINA_REGION
    MODEL_ID = "arn:aws:bedrock:us-east-1:538830382271:inference-profile/us.anthropic.claude-3-7-sonnet-20250219-v1:0" 

    prompt = "Vi·∫øt m·ªôt ƒëo·∫°n vƒÉn ng·∫Øn v·ªÅ l·ª£i √≠ch c·ªßa AI trong y t·∫ø."

    # Kh·ªüi t·∫°o manager
    manager = BedrockModelManager(
        aws_access_key_id=AWS_ACCESS_KEY,
        aws_secret_access_key=AWS_SECRET_KEY,
        region_name=REGION,
        default_model_id=MODEL_ID
    )

    # G·ªçi m√¥ h√¨nh
    try:
        print("üöÄ ƒêang g·ªçi m√¥ h√¨nh Bedrock Claude 3...")
        result, thinking = manager.generate(prompt=prompt, enable_thinking=False, model_type='claude')
        print("\n‚úÖ K·∫øt qu·∫£ ph·∫£n h·ªìi:")
        print(f"Thinking: {thinking}")
        print(f"Result: {result}")
    except Exception as e:
        print(f"‚ùå L·ªói khi g·ªçi m√¥ h√¨nh: {e}")    

if __name__ == "__main__":
    main()